{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d113b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "sys.path.insert(0, '/vast/nj594/mimic_explain/fastshap_text')\n",
    "from surrogate import TextSurrogate\n",
    "\n",
    "sys.path.insert(0, '/vast/nj594/xai/helpers')\n",
    "from evaluate import evaluate_mimic as evaluate\n",
    "from fastshap_dkl import FastSHAP_TEXT as FastSHAP\n",
    "\n",
    "# IMPORTANT: SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "os.environ['PYTHONHASHSEED'] = str(420)\n",
    "import random\n",
    "random.seed(420)\n",
    "np.random.seed(420)\n",
    "tf.random.set_seed(420)\n",
    "\n",
    "\n",
    "#Select GPU\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Command Line Arguements\n",
    "parser = argparse.ArgumentParser(description='REAL-X Mimic Experiment')\n",
    "parser.add_argument('--arg_file', type=str, default='', metavar='a',\n",
    "                    help='Path to File with Grid Search Arguments')\n",
    "parser.add_argument('--index', type=int, default=9999, metavar='i',\n",
    "                    help='Index for Job Array')\n",
    "parser.add_argument('--verbose', type=int, default=1, metavar='v',\n",
    "                    help='Prints Outputs')\n",
    "args = parser.parse_args()\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Get Index (Either from argument or from SLURM JOB ARRAY)\n",
    "if 'SLURM_ARRAY_TASK_ID' in os.environ:\n",
    "    args.index = int(os.environ['SLURM_ARRAY_TASK_ID'])\n",
    "    print('SLURM_ARRAY_TASK_ID found..., using index %s' % args.index)\n",
    "else:\n",
    "    print('no SLURM_ARRAY_TASK_ID... using index %s' % args.index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf9f542",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 16:38:49.938090: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-05 16:38:49.938629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 32252 MB memory:  -> device: 0, name: Vega 20, pci bus id: 0000:c5:00.0\n",
      "2022-05-05 16:38:50.819989: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-05-05 16:38:50.838798: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:38:50.841328: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:38:51.294670: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:38:51.296997: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:38:51.744726: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:38:51.746870: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:38:52.202139: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:38:52.204326: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "Some layers from the model checkpoint at surrogate/surrogate were not used when initializing TFBertForSequenceClassification: ['dropout_151']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at surrogate/surrogate.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./model/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: ({'input_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[  101,  3058,  1997, ..., 17850, 12104,   102],\n",
      "       [  101, 18583,  1012, ..., 10210,  7941,   102],\n",
      "       [  101,  2089, 16755, ...,  6292,  2089,   102],\n",
      "       ...,\n",
      "       [  101,  3460,  2012, ...,  2566,  2154,   102],\n",
      "       [  101,  1997,  3052, ...,  2590,  2008,   102],\n",
      "       [  101,  3058,  1997, ...,  5219,  2006,   102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(16, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(16, 2), dtype=int32, numpy=\n",
      "array([[0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1]], dtype=int32)>)\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'input_ids': <tf.Tensor 'IteratorGetNext:1' shape=(None, 128) dtype=int32>, 'attention_mask': <tf.Tensor 'IteratorGetNext:0' shape=(None, 128) dtype=int32>, 'token_type_ids': <tf.Tensor 'IteratorGetNext:2' shape=(None, 128) dtype=int32>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 16:39:03.224183: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:39:03.226407: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:39:03.349671: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:39:03.351923: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:39:03.358973: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:39:03.361071: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:39:08.570973: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:39:23.949413: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:39:23.951612: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:39:23.958687: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 16:39:23.961000: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "Some layers from the model checkpoint at ./model/model were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./model/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Get Arguments\n",
    "index = 0\n",
    "arg_file = 'fastshap/arg_file.pkl'\n",
    "with open(arg_file, \"rb\") as arg_file:\n",
    "    arg_file = pickle.load(arg_file)\n",
    "\n",
    "arg_file = arg_file[index]\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#Set Model Dir\n",
    "method = 'fastshap-dkl'\n",
    "run = str(index)\n",
    "model_dir = os.path.join(method, run)\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#Load Data \n",
    "\n",
    "data_dir = './data'\n",
    "label_list = [0, 1]\n",
    "max_seq_length = 128\n",
    "num_classes = len(label_list)\n",
    "\n",
    "### Initialize Tokenizer\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "mask_token = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "\n",
    "### Load\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train_dataset')\n",
    "val_dir = os.path.join(data_dir, 'val_dataset')\n",
    "test_dir = os.path.join(data_dir, 'test_dataset')\n",
    "\n",
    "element_spec = ({'input_ids': tf.TensorSpec(shape=(128,), dtype=tf.int32, name=None),\n",
    "                 'attention_mask': tf.TensorSpec(shape=(128,), dtype=tf.int32, name=None),\n",
    "                 'token_type_ids': tf.TensorSpec(shape=(128,), dtype=tf.int32, name=None)},\n",
    "                tf.TensorSpec(shape=(2,), dtype=tf.int32, name=None))\n",
    "\n",
    "train_data = tf.data.experimental.load(train_dir, element_spec)\n",
    "val_data = tf.data.experimental.load(val_dir, element_spec)\n",
    "test_data = tf.data.experimental.load(test_dir, element_spec)\n",
    "X_test = np.vstack([x[0]['input_ids'].numpy() for x in test_data])\n",
    "X_val = np.vstack([x[0]['input_ids'].numpy() for x in val_data])\n",
    "y_test = np.vstack([y.numpy() for x,y in test_data])\n",
    "y_val = np.vstack([y.numpy() for x,y in val_data])\n",
    "\n",
    "### Batch\n",
    "\n",
    "batch_size = 16\n",
    "train_data = train_data.shuffle(20000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_data = val_data.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_data = test_data.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Load Surrogate\n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "surrogate_model = TFBertForSequenceClassification.from_pretrained('surrogate/surrogate')    \n",
    "surrogate = TextSurrogate(surrogate_model = surrogate_model,\n",
    "                          seq_length = max_seq_length,\n",
    "                          baseline = mask_token)\n",
    "\n",
    "### Get Predicted Class\n",
    "from transformers import TFBertForSequenceClassification\n",
    "bert_model='./model/model'\n",
    "base_model = TFBertForSequenceClassification.from_pretrained(bert_model)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(tf.keras.layers.Lambda(lambda x: x.logits))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "for x in test_data:\n",
    "    model(x)\n",
    "    break\n",
    "\n",
    "model.trainable = False\n",
    "\n",
    "preds = model.predict(test_data)\n",
    "preds_discrete = np.eye(2)[preds.argmax(1)]\n",
    "\n",
    "preds_val = model.predict(val_data)\n",
    "preds_discrete_val = np.eye(2)[preds_val.argmax(1)]\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# FastSHAP\n",
    "\n",
    "### Specify Explainer Architecture\n",
    "\n",
    "from transformers.models.bert.modeling_tf_bert import TFBertPredictionHeadTransform\n",
    "bert_model='./model/model'\n",
    "base_model = TFBertForSequenceClassification.from_pretrained(bert_model)\n",
    "\n",
    "bert_main = base_model.layers[0]\n",
    "\n",
    "inputs = {}\n",
    "for k, v in train_data.unbatch().element_spec[0].items():\n",
    "    inputs[k] = tf.keras.layers.Input(shape = v.shape, name = k, dtype = v.dtype) \n",
    "input_key = [k for k in train_data.element_spec[0].keys() if 'input' in k.lower()][0] \n",
    "\n",
    "bert_out = bert_main(inputs)\n",
    "\n",
    "net = TFBertPredictionHeadTransform(config = bert_main.config)(bert_out[0])\n",
    "out = Dense(1)(net)\n",
    "\n",
    "explainer = Model(inputs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20e932c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import fastshap_dkl\n",
    "reload(fastshap_dkl)\n",
    "from fastshap_dkl import FastSHAP_TEXT as FastSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18ffbaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FastSHAP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 17:00:43.376347: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 17:00:43.380299: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 17:00:43.382837: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n"
     ]
    }
   ],
   "source": [
    "print('Training FastSHAP')\n",
    "fastshap = FastSHAP(explainer = explainer,\n",
    "                    imputer = surrogate,\n",
    "                    baseline = mask_token,\n",
    "                    normalization=arg_file['normalization'],\n",
    "                    link='identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a19332bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_2/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_2/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 17:01:08.721465: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 17:01:09.089850: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n",
      "2022-05-05 17:01:09.104219: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:507] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 106/1641 [>.............................] - ETA: 10:00 - loss: 22.7397 - shap_loss: 22.7397"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2730116/1514755636.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                \u001b[0mlookback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lookback'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                model_dir = model_dir)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vast/nj594/xai/helpers/fastshap_dkl.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, val_data, batch_size, num_samples, max_epochs, validation_batch_size, lr, min_lr, lr_factor, eff_lambda, paired_sampling, lookback, verbose, model_dir)\u001b[0m\n\u001b[1;32m    698\u001b[0m                                      \u001b[0mvalidation_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m                                      \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCALLBACKS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                                      verbose=verbose)\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_shap_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "fastshap.train(train_data = train_data,\n",
    "               val_data = val_data,\n",
    "               batch_size = arg_file['batch_size'],\n",
    "               num_samples = arg_file['num_samples'],\n",
    "               max_epochs = 10,\n",
    "               validation_batch_size = arg_file['batch_size'],\n",
    "               lr=arg_file['lr'],\n",
    "               min_lr=1e-5,\n",
    "               lr_factor=0.9,\n",
    "               eff_lambda=arg_file['eff_lambda'],\n",
    "               paired_sampling=arg_file['paired_sampling'],\n",
    "               lookback=arg_file['lookback'],\n",
    "               verbose=1, \n",
    "               model_dir = model_dir)\n",
    "training_time = time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'training_time.pkl'), 'wb') as f:\n",
    "    pickle.dump(training_time, f)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Explain w/ FastSHAP\n",
    "\n",
    "## TEST\n",
    "print('Explaining Test')\n",
    "\n",
    "### Explain\n",
    "\n",
    "t = time.time()\n",
    "shap_values = fastshap.shap_values(test_data).squeeze()\n",
    "explaining_time = time.time() - t\n",
    "\n",
    "### Save\n",
    "\n",
    "with open(os.path.join(model_dir, 'explaining_time.pkl'), 'wb') as f:\n",
    "    pickle.dump(explaining_time, f)\n",
    "\n",
    "with open(os.path.join(model_dir, 'shap_values.pkl'), 'wb') as f:\n",
    "    pickle.dump(shap_values, f)\n",
    "\n",
    "## VAL\n",
    "print('Explaining Val')\n",
    "\n",
    "### Explain\n",
    "\n",
    "t = time.time()\n",
    "shap_values_val = fastshap.shap_values(val_data).squeeze()\n",
    "explaining_time_val = time.time() - t\n",
    "\n",
    "### Save\n",
    "\n",
    "with open(os.path.join(model_dir, 'explaining_time_val.pkl'), 'wb') as f:\n",
    "    pickle.dump(explaining_time_val, f)\n",
    "\n",
    "with open(os.path.join(model_dir, 'shap_values_val.pkl'), 'wb') as f:\n",
    "    pickle.dump(shap_values_val, f)\n",
    "\n",
    "loss = np.min(fastshap.val_losses)\n",
    "with open(os.path.join(model_dir, 'loss.pkl'), 'wb') as f:\n",
    "    pickle.dump(loss, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "### Add Small Random Noise to prevent ties\n",
    "#### TEST\n",
    "shap_values += (np.random.random(shap_values.shape) * 1e-7)\n",
    "#### VAL\n",
    "shap_values_val += (np.random.random(shap_values_val.shape) * 1e-7)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "### Load DataFrame\n",
    "df_test = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(data_dir, \"val.csv\"))\n",
    "\n",
    "### Load Evaluator\n",
    "evaluator_base = TFBertForSequenceClassification.from_pretrained('evaluation/evaluator-data/surrogate')    \n",
    "\n",
    "evaluator_model = tf.keras.models.Sequential()\n",
    "evaluator_model.add(evaluator_base)\n",
    "evaluator_model.add(tf.keras.layers.Lambda(lambda x: x.logits))\n",
    "evaluator_model.add(tf.keras.layers.Activation('softmax'))\n",
    "for x in test_data:\n",
    "    evaluator_model(x)\n",
    "    break\n",
    "evaluator_model.summary()\n",
    "\n",
    "def eval_model(x):\n",
    "    attention_mask = np.ones_like(x).astype(int)\n",
    "    token_type_ids = np.zeros_like(x).astype(int)\n",
    "    \n",
    "    input_ = dict(\n",
    "        input_ids = x.astype(int),\n",
    "        attention_mask = attention_mask,\n",
    "        token_type_ids = token_type_ids,\n",
    "    )\n",
    "    \n",
    "    return evaluator_model.predict(input_)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#### Retrospective Evaluation ####\n",
    "\n",
    "# Exclusion\n",
    "retro_ex_val = evaluate(df_val.copy(), X_val, shap_values_val, evaluator_model, y_val, y_val, \n",
    "                        mode = 'exclude', method = method, mask_token=mask_token)\n",
    "retro_ex_test = evaluate(df_test.copy(), X_test, shap_values, evaluator_model, y_test, y_test, \n",
    "                         mode = 'exclude', method = method, mask_token=mask_token)\n",
    "\n",
    "# Inclusion\n",
    "retro_in_val = evaluate(df_val.copy(), X_val, shap_values_val, evaluator_model, y_val, y_val, \n",
    "                        mode = 'include', method = method, mask_token=mask_token)\n",
    "retro_in_test = evaluate(df_test.copy(), X_test, shap_values, evaluator_model, y_test, y_test, \n",
    "                         mode = 'include', method = method, mask_token=mask_token)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#### Prospective Evaluation ####\n",
    "\n",
    "# Exclusion\n",
    "pro_ex_val = evaluate(df_val.copy(), X_val, shap_values_val, evaluator_model, preds_discrete_val, y_val, \n",
    "                        mode = 'exclude', method = method, mask_token=mask_token)\n",
    "pro_ex_test = evaluate(df_test.copy(), X_test, shap_values, evaluator_model, preds_discrete, y_test, \n",
    "                         mode = 'exclude', method = method, mask_token=mask_token)\n",
    "\n",
    "# Inclusion\n",
    "pro_in_val = evaluate(df_val.copy(), X_val, shap_values_val, evaluator_model, preds_discrete_val, y_val, \n",
    "                        mode = 'include', method = method, mask_token=mask_token)\n",
    "pro_in_test = evaluate(df_test.copy(), X_test, shap_values, evaluator_model, preds_discrete, y_test, \n",
    "                         mode = 'include', method = method, mask_token=mask_token)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# Combine Results\n",
    "tags = ['retro_ex_val','retro_ex_test','retro_in_val','retro_in_test', \n",
    "        'pro_ex_val','pro_ex_test','pro_in_val','pro_in_test']\n",
    "result_list = [retro_ex_val,retro_ex_test,retro_in_val,retro_in_test,\n",
    "               pro_ex_val,pro_ex_test,pro_in_val,pro_in_test]\n",
    "\n",
    "results = {}\n",
    "for res, tag  in zip(result_list, tags):\n",
    "    res = {k+'-'+tag:v for k,v in res.items()}\n",
    "    results = {**results, **res}\n",
    "\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Save\n",
    "\n",
    "### Create Results Dictionary\n",
    "header = [\"model_dir\", \"lr\", \"epochs\", \"batch_size\", \"lookback\", \n",
    "          \"num_samples\", \"paired_sampling\", \"eff_lambda\", \"normalization\", \n",
    "          \"training_time\", \"explaining_time\"]\n",
    "metrics = ['AUC_acc','AUC_auroc','AUC_log_likelihood','AUC_log_odds']\n",
    "for tag in tags:\n",
    "    header += [x+'-'+tag for x in metrics]\n",
    "    \n",
    "results = {**results, **arg_file}\n",
    "results['model_dir'] = model_dir\n",
    "results[\"explaining_time\"] = explaining_time\n",
    "results[\"training_time\"] = training_time\n",
    "results = {k:v for k,v in results.items() if k in header}\n",
    "\n",
    "### Convert to DataFrame\n",
    "results_df = pd.DataFrame(results, index=[0])\n",
    "results_df = results_df[header]\n",
    "\n",
    "### Append DataFrame to csv\n",
    "results_path = method+'/results.csv'\n",
    "if os.path.exists(results_path):\n",
    "    results_df.to_csv(results_path, mode='a',  header=False)\n",
    "else:\n",
    "    results_df.to_csv(results_path, mode='w',  header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
