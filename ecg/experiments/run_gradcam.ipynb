{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e730d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 19:23:23.715251: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs\n",
      "2022-04-16 19:23:23.715274: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-16 19:23:23.715290: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cs042.hpc.nyu.edu): /proc/driver/nvidia/version does not exist\n",
      "2022-04-16 19:23:23.715470: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-16 19:23:23.721368: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2900000000 Hz\n",
      "2022-04-16 19:23:23.721477: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b514eb80f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-04-16 19:23:23.721484: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1000, 1)]         0         \n",
      "_________________________________________________________________\n",
      "functional_1 (Functional)    (None, 4, 256)            5246112   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 5,246,626\n",
      "Trainable params: 5,238,882\n",
      "Non-trainable params: 7,744\n",
      "_________________________________________________________________\n",
      "/scratch/nj594/xai/ecg/experiments/model/model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (Input, Dense, GlobalAveragePooling1D, Conv1D, \n",
    "                                     AveragePooling1D, UpSampling1D, Flatten)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, '/scratch/nj594/xai/helpers')\n",
    "from evaluate import evaluate\n",
    "\n",
    "# IMPORTANT: SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "os.environ['PYTHONHASHSEED'] = str(420)\n",
    "import random\n",
    "random.seed(420)\n",
    "np.random.seed(420)\n",
    "tf.random.set_seed(420)\n",
    "\n",
    "\n",
    "# Get Arguments\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#Set Model Dir\n",
    "method = 'gradcam'\n",
    "model_dir = 'gradcam'\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Load Data\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "X_train = np.load(os.path.join(data_dir, 'X_train.npy'), allow_pickle=True)\n",
    "X_val = np.load(os.path.join(data_dir, 'X_val.npy'), allow_pickle=True)\n",
    "X_test = np.load(os.path.join(data_dir, 'X_test.npy'), allow_pickle=True)\n",
    "\n",
    "y_train = np.load(os.path.join(data_dir, 'y_train.npy'), allow_pickle=True)\n",
    "y_val = np.load(os.path.join(data_dir, 'y_val.npy'), allow_pickle=True)\n",
    "y_test = np.load(os.path.join(data_dir, 'y_test.npy'), allow_pickle=True)\n",
    "\n",
    "preds_train = np.load(os.path.join(data_dir, 'predictions_train.npy'), allow_pickle=True)\n",
    "preds_discrete_train = np.eye(2)[preds_train.argmax(1)]\n",
    "preds_val = np.load(os.path.join(data_dir, 'predictions_val.npy'), allow_pickle=True)\n",
    "preds_discrete_val = np.eye(2)[preds_val.argmax(1)]\n",
    "preds = np.load(os.path.join(data_dir, 'predictions.npy'), allow_pickle=True)\n",
    "preds_discrete = np.eye(2)[preds.argmax(1)]\n",
    "\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Load Model\n",
    "\n",
    "params = {\n",
    "    #NN Hyperparameters\n",
    "    \"input_shape\": [1000, 1],\n",
    "    \"num_categories\": 2,\n",
    "    \"conv_subsample_lengths\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
    "    \"conv_filter_length\": 8,\n",
    "    \"conv_num_filters_start\": 32,\n",
    "    \"conv_init\": \"he_normal\",\n",
    "    \"conv_activation\": \"relu\",\n",
    "    \"conv_dropout\": 0.2,\n",
    "    \"conv_num_skip\": 2,\n",
    "    \"conv_increase_channels_at\": 4,\n",
    "    \"compile\": False,\n",
    "    \"is_regular_conv\": False,\n",
    "    \"is_by_time\": False,\n",
    "    \"is_by_lead\": False,\n",
    "    \"ecg_out_size\": 64,\n",
    "    \"nn_layer_sizes\" : None,\n",
    "    \"is_multiply_layer\": False, \n",
    "}\n",
    "\n",
    "#Stanford Model\n",
    "sys.path.insert(0, '/scratch/nj594/ecg/models/stanford')\n",
    "import network\n",
    "\n",
    "model_input = Input(shape=(1000,1))\n",
    "\n",
    "cnn = network.build_network(**params) \n",
    "cnn = Model(cnn.inputs, cnn.layers[-4].output)\n",
    "\n",
    "net = cnn(model_input)\n",
    "net = GlobalAveragePooling1D()(net)\n",
    "out = Dense(2, activation='softmax')(net)\n",
    "\n",
    "model = Model(model_input, out)\n",
    "model.summary()\n",
    "\n",
    "# Model Checkpointing\n",
    "save_dir = 'model'\n",
    "model_dir = os.path.join(os.getcwd(), save_dir)\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "model_weights_path = os.path.join(model_dir, 'model_weights.h5')\n",
    "\n",
    "# Get Checkpointed Model\n",
    "print(model_weights_path)\n",
    "model.load_weights(model_weights_path)\n",
    "model.trainable = False\n",
    "\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(1e-3)\n",
    "METRICS = [ \n",
    "  tf.keras.metrics.AUC(name='auroc'),\n",
    "  tf.keras.metrics.AUC(curve='PR', name='auprc'),\n",
    "  tf.keras.metrics.TopKCategoricalAccuracy(k=1, name='accuracy'),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=OPTIMIZER,\n",
    "    metrics=METRICS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-claim",
   "metadata": {},
   "source": [
    "# Grad CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "divine-suffering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surgically appending layer : <tensorflow.python.keras.layers.pooling.GlobalAveragePooling1D object at 0x7fcc3c029c10>\n",
      "Surgically appending layer : <tensorflow.python.keras.layers.core.Dense object at 0x7fcc3c0345d0>\n"
     ]
    }
   ],
   "source": [
    "superpixel_size = 8\n",
    "\n",
    "#### Get Model for Feature Map ####\n",
    "submodel_index, submodel = 1, model.layers[1]\n",
    "x = submodel.outputs[0]\n",
    "for layer_index in range(submodel_index+1, len(model.layers)):\n",
    "    extracted_layer = model.layers[layer_index]\n",
    "    print('Surgically appending layer : '+str(extracted_layer))\n",
    "    x = extracted_layer(x)\n",
    "flat_model = Model(inputs=submodel.inputs, outputs=x)\n",
    "\n",
    "grad_model = tf.keras.models.Model(\n",
    "    flat_model.inputs, [flat_model.layers[67].get_output_at(0), flat_model.output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1ed2b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [01:06<00:00,  1.50s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [01:05<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "def ponderate_output(output, grad):\n",
    "    \"\"\"\n",
    "    Perform the ponderation of filters output with respect to average of gradients values.\n",
    "    \"\"\"\n",
    "    weights = tf.reduce_mean(grad, 0)\n",
    "\n",
    "    # Perform ponderated sum : w_i * output[:, i]\n",
    "    cam = tf.reduce_sum(tf.multiply(weights, output), axis=-1)\n",
    "\n",
    "    return cam\n",
    "\n",
    "explanations = []\n",
    "for y_class in range(y_test.shape[1]):\n",
    "    cams = []\n",
    "    for i in tqdm(range(44)): #mini-batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            if (i+1)*50 > len(X_test):\n",
    "                end = len(X_test)+1\n",
    "            else:\n",
    "                end = (i+1)*50\n",
    "            inputs = tf.cast(X_test[i*50:end], tf.float32)\n",
    "            tape.watch(inputs)\n",
    "            conv_outputs, predictions = grad_model(inputs)\n",
    "            loss = predictions[:, y_class]\n",
    "\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "        grads = (\n",
    "             tf.cast(conv_outputs > 0, \"float32\")\n",
    "             * tf.cast(grads > 0, \"float32\")\n",
    "             * grads\n",
    "         )\n",
    "        \n",
    "        \n",
    "        cam = UpSampling1D(size=superpixel_size)(\n",
    "            tf.expand_dims(tf.stack([ponderate_output(output, grad) for output, grad in zip(conv_outputs, grads)]), -1)\n",
    "        )\n",
    "#         cam = tf.maximum(cam, 0)\n",
    "        cams.append(cam)\n",
    "\n",
    "    cams = tf.concat(cams, 0).numpy()\n",
    "    explanations.append(cams)\n",
    "\n",
    "explaining_time = time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c6d51b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(method, 'explanations.pkl'), 'wb') as f:\n",
    "    pickle.dump(explanations, f)\n",
    "\n",
    "with open(os.path.join(method, 'explaining_time.pkl'), 'wb') as f:\n",
    "    pickle.dump(explaining_time, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fe61c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ece5858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Load Evaluator Model\n",
    "\n",
    "eval_dir = os.path.join(os.getcwd(), 'evaluation', 'evaluator-data')\n",
    "evaluator_model = tf.keras.models.load_model(os.path.join(eval_dir, 'surrogate.h5'))\n",
    "\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(1e-3)\n",
    "METRICS = [ \n",
    "  tf.keras.metrics.AUC(name='auroc'),\n",
    "  tf.keras.metrics.AUC(curve='PR', name='auprc'),\n",
    "  tf.keras.metrics.TopKCategoricalAccuracy(k=1, name='accuracy'),\n",
    "]\n",
    "\n",
    "evaluator_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=OPTIMIZER,\n",
    "    metrics=METRICS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0ecf8c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "99\n",
      "95\n",
      "90\n",
      "85\n",
      "75\n",
      "50\n",
      "25\n",
      "15\n",
      "10\n",
      "5\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#### Retrospective Evaluation ####\n",
    "\n",
    "# Exclusion\n",
    "retro_ex_test = evaluate(X_test, explanations, evaluator_model, y_test, y_test, \n",
    "                         mode = 'exclude', method = method)\n",
    "\n",
    "# Inclusion\n",
    "retro_in_test = evaluate(X_test, explanations, evaluator_model, y_test, y_test, \n",
    "                         mode = 'include', method = method)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#### Prospective Evaluation ####\n",
    "\n",
    "# Exclusion\n",
    "pro_ex_test = evaluate(X_test, explanations, evaluator_model, preds_discrete, y_test, \n",
    "                         mode = 'exclude', method = method)\n",
    "\n",
    "# Inclusion\n",
    "pro_in_test = evaluate(X_test, explanations, evaluator_model, preds_discrete, y_test, \n",
    "                         mode = 'include', method = method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0d8c2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# Combine Results\n",
    "tags = ['retro_ex','retro_in','pro_ex','pro_in']\n",
    "result_list = [retro_ex_test,retro_in_test,pro_ex_test,pro_in_test]\n",
    "\n",
    "results = {}\n",
    "for res, tag  in zip(result_list, tags):\n",
    "    res = {k+'-'+tag:v for k,v in res.items()}\n",
    "    results = {**results, **res}\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Save\n",
    "\n",
    "### Create Results Dictionary\n",
    "header = [\"model_dir\", \"explaining_time\"]\n",
    "metrics = ['AUC_acc','AUC_auroc','AUC_log_likelihood','AUC_log_odds']\n",
    "for tag in tags:\n",
    "    header += [x+'-'+tag for x in metrics]\n",
    "    \n",
    "results['model_dir'] = model_dir\n",
    "results[\"explaining_time\"] = explaining_time\n",
    "results = {k:v for k,v in results.items() if k in header}\n",
    "\n",
    "### Convert to DataFrame\n",
    "results_df = pd.DataFrame(results, index=[0])\n",
    "results_df = results_df[header]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3e49f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Append DataFrame to csv\n",
    "results_path = method+'/results.csv'\n",
    "if os.path.exists(results_path):\n",
    "    results_df.to_csv(results_path, mode='a',  header=False)\n",
    "else:\n",
    "    results_df.to_csv(results_path, mode='w',  header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
